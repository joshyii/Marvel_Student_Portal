{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    30000 non-null  object\n",
      " 1   class   30000 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 468.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Dataset Link (https://www.kaggle.com/datasets/nikhileswarkomati/suicide-watch/data)\n",
    "csv_file_path = 'Suicide_Detection.csv'\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.drop(df.columns[0], axis=1, inplace=True)\n",
    "df = df.head(30000)\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text        class\n",
      "0  Ex Wife Threatening SuicideRecently I left my ...      suicide\n",
      "1  Am I weird I dont get affected by compliments ...  non-suicide\n",
      "2  Finally is almost over So I can never hear  ha...  non-suicide\n",
      "3          i need helpjust help me im crying so hard      suicide\n",
      "4  Im so lostHello my name is Adam  and Ive been ...      suicide\n"
     ]
    }
   ],
   "source": [
    "def pre_processing(data):\n",
    "    #remove digit\n",
    "    data = data.apply(lambda x: ''.join([char for char in x if not char.isdigit()]))\n",
    "\n",
    "    #remove empty spaces\n",
    "    data = data.apply(lambda x: \" \".join(x.split()))\n",
    "\n",
    "    #remove puncutations except ! bcos ! can show emotion\n",
    "    data = data.apply(lambda x: re.sub(r'[^\\w\\s!]', '', x))\n",
    "\n",
    "    #Remove emoji\n",
    "    data = data.apply(remove_emoji)\n",
    "\n",
    "    return data\n",
    "\n",
    "df['text'] = pre_processing(df['text'])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  class\n",
      "0  Ex Wife Threatening SuicideRecently I left my ...      1\n",
      "1  Am I weird I dont get affected by compliments ...      0\n",
      "2  Finally is almost over So I can never hear  ha...      0\n",
      "3          i need helpjust help me im crying so hard      1\n",
      "4  Im so lostHello my name is Adam  and Ive been ...      1\n"
     ]
    }
   ],
   "source": [
    "#Map output to be binary 1 or 0\n",
    "mapping = {'suicide': 1, 'non-suicide': 0}\n",
    "df['class'] = df['class'].map(mapping)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2    29     4 ...    10   171   402]\n",
      " [   49     2   451 ...     0     0     0]\n",
      " [  289    15   275 ...     0     0     0]\n",
      " ...\n",
      " [  393  1059  9316 ...    97    34   108]\n",
      " [   18 10053     9 ...    15   352     9]\n",
      " [  800     1    65 ...     0     0     0]]\n",
      "[1 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 50\n",
    "\n",
    "X = df['text'].values\n",
    "y = df['class'].values\n",
    "y=np.array(y)\n",
    "\n",
    "\n",
    "# # Tokenize the entire dataset\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X)\n",
    "\n",
    "# Convert text to sequences of integers\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "X = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")\n",
    "\n",
    "print(X)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 8  # Number of attention heads\n",
    "ff_dim = 128  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dropout(0.35)(x)\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1:\n",
      "Epoch 1/3\n",
      "750/750 [==============================] - 42s 49ms/step - loss: 0.3588 - accuracy: 0.8545 - val_loss: 0.2738 - val_accuracy: 0.8840\n",
      "Epoch 2/3\n",
      "750/750 [==============================] - 11s 15ms/step - loss: 0.2311 - accuracy: 0.9160 - val_loss: 0.2725 - val_accuracy: 0.8922\n",
      "Epoch 3/3\n",
      "750/750 [==============================] - 12s 17ms/step - loss: 0.1851 - accuracy: 0.9352 - val_loss: 0.2810 - val_accuracy: 0.8948\n",
      "188/188 [==============================] - 2s 7ms/step - loss: 0.2810 - accuracy: 0.8948\n",
      "Validation Metrics: [0.2809550166130066, 0.8948333263397217]\n",
      "Fold 2:\n",
      "Epoch 1/3\n",
      "750/750 [==============================] - 13s 18ms/step - loss: 0.1853 - accuracy: 0.9358 - val_loss: 0.1520 - val_accuracy: 0.9392\n",
      "Epoch 2/3\n",
      "750/750 [==============================] - 14s 18ms/step - loss: 0.1399 - accuracy: 0.9508 - val_loss: 0.1513 - val_accuracy: 0.9395\n",
      "Epoch 3/3\n",
      "750/750 [==============================] - 14s 18ms/step - loss: 0.1117 - accuracy: 0.9614 - val_loss: 0.2283 - val_accuracy: 0.9272\n",
      "188/188 [==============================] - 2s 11ms/step - loss: 0.2283 - accuracy: 0.9272\n",
      "Validation Metrics: [0.22827109694480896, 0.9271666407585144]\n",
      "Fold 3:\n",
      "Epoch 1/3\n",
      "750/750 [==============================] - 13s 18ms/step - loss: 0.1159 - accuracy: 0.9577 - val_loss: 0.0839 - val_accuracy: 0.9700\n",
      "Epoch 2/3\n",
      "750/750 [==============================] - 13s 18ms/step - loss: 0.0864 - accuracy: 0.9667 - val_loss: 0.1019 - val_accuracy: 0.9578\n",
      "Epoch 3/3\n",
      "750/750 [==============================] - 13s 18ms/step - loss: 0.0694 - accuracy: 0.9736 - val_loss: 0.1291 - val_accuracy: 0.9550\n",
      "188/188 [==============================] - 2s 11ms/step - loss: 0.1291 - accuracy: 0.9550\n",
      "Validation Metrics: [0.12907998263835907, 0.9549999833106995]\n",
      "Fold 4:\n",
      "Epoch 1/3\n",
      "750/750 [==============================] - 14s 18ms/step - loss: 0.0776 - accuracy: 0.9704 - val_loss: 0.0687 - val_accuracy: 0.9705\n",
      "Epoch 2/3\n",
      "750/750 [==============================] - 14s 19ms/step - loss: 0.0549 - accuracy: 0.9777 - val_loss: 0.0621 - val_accuracy: 0.9727\n",
      "Epoch 3/3\n",
      "750/750 [==============================] - 13s 17ms/step - loss: 0.0450 - accuracy: 0.9805 - val_loss: 0.0772 - val_accuracy: 0.9740\n",
      "188/188 [==============================] - 1s 7ms/step - loss: 0.0772 - accuracy: 0.9740\n",
      "Validation Metrics: [0.07715890556573868, 0.9739999771118164]\n",
      "Fold 5:\n",
      "Epoch 1/3\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.0548 - accuracy: 0.9778 - val_loss: 0.0300 - val_accuracy: 0.9882\n",
      "Epoch 2/3\n",
      "750/750 [==============================] - 12s 16ms/step - loss: 0.0359 - accuracy: 0.9858 - val_loss: 0.0396 - val_accuracy: 0.9860\n",
      "Epoch 3/3\n",
      "750/750 [==============================] - 12s 15ms/step - loss: 0.0298 - accuracy: 0.9886 - val_loss: 0.0439 - val_accuracy: 0.9823\n",
      "188/188 [==============================] - 2s 8ms/step - loss: 0.0439 - accuracy: 0.9823\n",
      "Validation Metrics: [0.0438835434615612, 0.9823333621025085]\n",
      "Average loss:  0.1518697090446949\n",
      "Average accuracy:  0.9466666579246521\n"
     ]
    }
   ],
   "source": [
    "# Define k-fold cross-validation\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "loss_total = 0\n",
    "accuracy_total = 0\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kfold.split(X, y)):\n",
    "    print(f\"Fold {fold + 1}:\")\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_val = X[train_idx], X[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # # Tokenize the data\n",
    "    # X_train_tokenized = Tokenizer(X_train, truncation=True, padding=True)\n",
    "    # X_val_tokenized = Tokenizer(X_val, truncation=True, padding=True)\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "    # Evaluate the model\n",
    "    metrics = model.evaluate(X_val, y_val)\n",
    "    print(f\"Validation Metrics: {metrics}\")\n",
    "    loss_total += metrics[0]\n",
    "    accuracy_total += metrics[1]\n",
    "\n",
    "print(\"Average loss: \", loss_total/5)\n",
    "print(\"Average accuracy: \", accuracy_total/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MentalAI\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: MentalAI\\assets\n"
     ]
    }
   ],
   "source": [
    "#Save Model\n",
    "model.save('MentalAI', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo - 6 test texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "1/1 [==============================] - 0s 306ms/step\n",
      "Predicted Class: suicidal\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Predicted Class: suicidal\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "Predicted Class: suicidal\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Predicted Class: suicidal\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "Predicted Class: suicidal\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Predicted Class: non-suicidal\n",
      "Stored 'MentalAIOutput' (float64)\n"
     ]
    }
   ],
   "source": [
    "text_array=[\"Every time my partner gets angry for anything, she takes it out on me. Nothing I do is right, and once she's mad, she calls me all kinds of names and is verbally abusive. She says it isn't abuse, it's just angry verbal bashing, and that it's different. It gets worse each time. The names are very vulgar now.\", \"I am depressed and wanna kill myself, everything seems hopeless now\", \"Please don't stop me, there's no turning back anymore\", \"I think I am okay, but not really\", \"I want to rest, I am overwhelmed\", \"I ate some food just now\"]\n",
    "print(len(text_array))\n",
    "\n",
    "def MentalAI(text):\n",
    "    text = pre_processing(pd.Series(text))\n",
    "\n",
    "    sequences = tokenizer.texts_to_sequences(text)\n",
    "\n",
    "    text = pad_sequences(sequences, maxlen=maxlen, padding=\"post\")\n",
    "    predictions = model.predict(text)\n",
    "    predicted_class = \"non-suicidal\" if predictions[0][0] > 0.5 else \"suicidal\"\n",
    "    print(f\"Predicted Class: {predicted_class}\")\n",
    "\n",
    "    return predictions\n",
    "\n",
    "sum = 0\n",
    "\n",
    "# Iterate through every text messages\n",
    "for i in range(len(text_array)):\n",
    "    result = MentalAI(text_array[i])\n",
    "    sum += result[0, 1]\n",
    "\n",
    "MentalAIOutput = sum/len(text_array)\n",
    "%store MentalAIOutput\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
